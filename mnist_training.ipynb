{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "mnist_training.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "0rAI1mqpTuSh"
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras import layers\n",
        "from keras.layers import Conv2D,MaxPooling2D,Flatten,Dropout,Dense,BatchNormalization\n",
        "from keras.datasets import mnist\n",
        "import numpy as np\n",
        "from tensorflow.python.keras.utils.np_utils import to_categorical\n",
        "import matplotlib.pyplot as plt\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7YhT2lzUDqA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97ebb2ad-0e0c-40ca-cc85-f199d92a5b18"
      },
      "source": [
        "# load MNIST dataset from keras\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()\n",
        "\n",
        "# normalize data between 0 and 1\n",
        "train_images=train_images.astype('float32')\n",
        "test_images=test_images.astype('float32')\n",
        "train_images = train_images / 255\n",
        "test_images = test_images / 255\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QbhXRCjgTLv8"
      },
      "source": [
        "x_train=train_images.reshape(train_images.shape[0],784)\n",
        "# the code below takes the same specific amount of images from every class, accoriding to num_of_indices variable\n",
        "# i will use them in the classifier\n",
        "num_of_indices = 20\n",
        "\n",
        "indices0 = [i for i in range(len(x_train)) if train_labels[i] == 0]\n",
        "indices0=indices0[:num_of_indices]\n",
        "indices1 = [i for i in range(len(x_train)) if train_labels[i] == 1]\n",
        "indices1=indices1[:num_of_indices]\n",
        "indices2 = [i for i in range(len(x_train)) if train_labels[i] == 2]\n",
        "indices2=indices2[:num_of_indices]\n",
        "indices3 = [i for i in range(len(x_train)) if train_labels[i] == 3]\n",
        "indices3=indices3[:num_of_indices]\n",
        "indices4 = [i for i in range(len(x_train)) if train_labels[i] == 4]\n",
        "indices4=indices4[:num_of_indices]\n",
        "indices5 = [i for i in range(len(x_train)) if train_labels[i] == 5]\n",
        "indices5=indices5[:num_of_indices]\n",
        "indices6 = [i for i in range(len(x_train)) if train_labels[i] == 6]\n",
        "indices6=indices6[:num_of_indices]\n",
        "indices7 = [i for i in range(len(x_train)) if train_labels[i] == 7]\n",
        "indices7=indices7[:num_of_indices]\n",
        "indices8 = [i for i in range(len(x_train)) if train_labels[i] == 8]\n",
        "indices8=indices8[:num_of_indices]\n",
        "indices9= [i for i in range(len(x_train)) if train_labels[i] == 9]\n",
        "indices9=indices9[:num_of_indices]\n",
        "# store every array with images from each class in one array\n",
        "indices=indices0+indices1+indices2+indices3+indices4+indices5+indices6+indices7+indices8+indices9\n",
        "indices.sort()\n",
        "#print(indices)\n",
        "#print(len(indices))\n",
        "#print(x_train[87])\n",
        "# clas images is the array that now holds the dataset to be used as input in the NN\n",
        "clas_images=x_train[[indices[:]],:]\n",
        "clas_images=np.reshape(clas_images,(len(indices),784))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yDuGpaSmS7Fu"
      },
      "source": [
        "# reshape my data to the shape needed to use it as input \n",
        "clas_images=np.reshape(clas_images,(len(clas_images),28,28,1))\n",
        "train_images=np.reshape(train_images,(len(train_images),28,28,1))\n",
        "test_images = np.reshape(test_images, (len(test_images), 28, 28, 1))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H8Yv1mLJUDFO"
      },
      "source": [
        "# take the labels of the images i chose in each class\n",
        "clas_labels=train_labels[indices[:]]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fr5BS2WsUKPc"
      },
      "source": [
        "# transorm labels to one hot encoding\n",
        "clas_labels=to_categorical(clas_labels, 10)\n",
        "tr_labels=to_categorical(train_labels, 10)\n",
        "te_labels=to_categorical(test_labels,10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x0hU60QDUSV6"
      },
      "source": [
        "# choose the first x data from my test set to test my model\n",
        "chosen_data = 100\n",
        "val_images=test_images[:chosen_data]\n",
        "val_labels=te_labels[:chosen_data]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vy0plo_iUGkD"
      },
      "source": [
        "# my neural network\n",
        "# first input shape is defined \n",
        "input_img = keras.Input(shape=(28, 28, 1))\n",
        "# initializer defined\n",
        "initializer=keras.initializers.GlorotNormal()\n",
        "# first convolutional layer with 32 neurons, kernel 3x3, activation function relu, same padding and as input the input_img\n",
        "conv1=layers.Conv2D(32,kernel_size=3,activation='relu',padding='same',kernel_initializer=initializer)(input_img)\n",
        "#first dropout layer\n",
        "d1=layers.Dropout(0.4)(conv1)\n",
        "#fisrt batch normalization layer \n",
        "b1=layers.BatchNormalization()(d1)\n",
        "# first 2d max pooling layer \n",
        "maxp1=layers.MaxPooling2D(pool_size=(2,2))(b1)\n",
        "\n",
        "conv2=layers.Conv2D(64,kernel_size=3,activation='relu',padding='same',kernel_initializer=initializer)(maxp1)\n",
        "d2=layers.Dropout(0.4)(conv2)\n",
        "b2=layers.BatchNormalization()(d2)\n",
        "maxp2=layers.MaxPooling2D(pool_size=(2,2))(b2)\n",
        "\n",
        "conv3=layers.Conv2D(128,kernel_size=3,activation='relu',padding='same',kernel_initializer=initializer)(maxp2)\n",
        "# output of the third convolutional layer has to be flattened to be passed into the dense layer\n",
        "flat=layers.Flatten()(conv3)\n",
        "# first dense layer with 100 neurons and sigmoid as activation function \n",
        "dense1=layers.Dense(100,activation='sigmoid',kernel_initializer=initializer)(flat)\n",
        "# second dense layer with 10 neurons (because we have 10 classes) and softmax as activation for the classification  \n",
        "final=layers.Dense(10,activation='softmax',kernel_initializer=initializer)(dense1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84dsV9vhUNNI"
      },
      "source": [
        "# here the model is defined, with claiming the input and the output\n",
        "model=keras.Model(input_img, final)\n",
        "#adam=keras.optimizers.Adam(learning_rate=0.000001)\n",
        "# the model here is compiled, using an optimizer, a loss function and metrics\n",
        "model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TOpB_lyvVEcz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 167
        },
        "outputId": "a0a33e8a-ceab-4e28-8d2c-9866d8b46bb0"
      },
      "source": [
        "# here the model is configured for training with parameteres such as the training images and labels\n",
        "# the number of epochs, the batch size and the testing data\n",
        "# the output is stored to history object in order to print the output later\n",
        "history=model.fit(train_images,tr_labels,epochs=10,batch_size=16,validation_data=(test_images,te_labels))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-735731566c8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhistory\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtr_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mte_labels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_OHEo_eVXCQ"
      },
      "source": [
        "# finding the overall test loss and accuracy\n",
        "score=model.evaluate(test_images,te_labels)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VxIuEPdI0D7l"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OUEEAYjhfHvp"
      },
      "source": [
        "# summarize history for accuracy\n",
        "plt.plot(history.history['accuracy'])\n",
        "plt.plot(history.history['val_accuracy'])\n",
        "plt.title('model accuracy')\n",
        "plt.ylabel('accuracy')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()\n",
        "# summarize history for loss\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "plt.title('model loss')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['train', 'test'], loc='upper left')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gp4oplddebvI"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}